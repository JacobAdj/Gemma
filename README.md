# Fine-tuning Gemma to understand Dutch numerals



## Introduction

Here I demonstrate fine-tuning the Gemma Large Language Model using Low Rank Adaptation (LoRA) along the lines decribed in this article:Fine-tune Gemma models in Keras using LoRA.

I use an example in Dutch language understanding: understand Dutch numerals written in words. This is a fairly complex problem on which Gemma performs poorly, as not only are the words in Dutch, but Dutch numerals are written in a different order than English ones and several words can be strung together in various ways.

For example the number 34 is written as vierendertig (vier = 4 , en = and, dertig = 30). 483 is written as vierhonderddrieentachtig (vier = 4 , honderd = 100 , drie = 3 , en = and, tachtig = 80 - but no 'en' between 'vierhonderd' and 'drieentachtig').


## Training data

Training data were pairs of instruction and response as follows:

```json
{"response": "607", "instruction": "zeshonderdzeven"}
{"response": "920", "instruction": "negenhonderdtwintig"}
{"response": "17", "instruction": "zeventien"}
{"response": "476", "instruction": "vierhonderdzesenzeventig"}
{"response": "626", "instruction": "zeshonderdzesentwintig"}
{"response": "303", "instruction": "driehonderddrie"}
{"response": "442", "instruction": "vierhonderdtweeenveertig"}
{"response": "800", "instruction": "achthonderd"}
{"response": "148", "instruction": "eenhonderdachtenveertig"}
{"response": "39", "instruction": "negenendertig"}
{"response": "483", "instruction": "vierhonderddrieentachtig"}
{"response": "779", "instruction": "zevenhonderdnegenenzeventig"}
...
```

The training data were generated by a Python program as follows:

```python
import random
import json

print("hi")

def getal_in_woorden(getal):
    eenheden = ["", "een", "twee", "drie", "vier", "vijf", "zes", "zeven", "acht", "negen"]
    tientallen = ["", "", "twintig", "dertig", "veertig", "vijftig", "zestig", "zeventig", "tachtig", "negentig"]

    miljoentallen = ""
    duizendtallen = ""
    if getal >= 1000000:
        miljoentallen = getal_naar_honderden(getal // 1000000) + "miljoen "
        getal = getal % 1000000

    if getal >= 1000:
        duizendtallen = getal_naar_honderden(getal // 1000) + "duizend "
        getal = getal % 1000

    return miljoentallen + duizendtallen + getal_naar_honderden(getal)

def getal_naar_honderden(num):
    eenheden = ["", "een", "twee", "drie", "vier", "vijf", "zes", "zeven", "acht", "negen", "tien"]
    tientallen = ["", "", "twintig", "dertig", "veertig", "vijftig", "zestig", "zeventig", "tachtig", "negentig"]
    res = ""

    if num >= 100:
        res = eenheden[num // 100] + "honderd"
        num = num % 100

    if 10 < num < 20:
        special_cases = {
            11: "elf", 12: "twaalf", 13: "dertien", 14: "veertien", 15: "vijftien", 
            16: "zestien", 17: "zeventien", 18: "achttien", 19: "negentien"
        }
        res += special_cases[num]
    elif num >= 20:
        if num % 10 != 0:
            res += eenheden[num % 10] + "en"
        res += tientallen[num // 10]
    # elif num == 10:
    #     res += eenheden[0]
    else:
        print('num', num)
        res += eenheden[num]

    return res


traindata = []

with open('meergetallendata.json', 'a') as outfile: 
 
    for d in range(1000):

        case = {}

        getal = random.randint(1, 1000)
        print('getal', getal)

        woorden = getal_in_woorden(getal)
        print(woorden)

        case['response'] = str(getal)
        case['instruction'] = woorden

        print(json.dumps(case))
        outfile.write(json.dumps(case) + '\n')
```

## Loading a pretrained model

We create a model using the Keras implementation of GemmaCausalLM, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.

Create the model using the from_preset method:     

```python
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma2_2b_en")
gemma_lm.summary()
```


## Fine-tuning

The pretrained model only produces some random-looking text from the Dutch numerals data.

To generate better responses, we fine-tune the model with Low Rank Adaptation (LoRA) using 1000 examples of correct data in the form {"instruction": "negenhonderdvierentwintig" , "response": "924" }.

The LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.

A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation. We use a LoRA rank of 4:

```python
gemma_lm.backbone.enable_lora(rank=4)
gemma_lm.summary()

# Limit the input sequence length to 27 
gemma_lm.preprocessor.sequence_length = 27
# Use AdamW (a common optimizer for transformer models).
optimizer = keras.optimizers.AdamW(
    learning_rate=5e-5,
    weight_decay=0.01,
)
# Exclude layernorm and bias terms from decay.
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

gemma_lm.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=optimizer,
    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
gemma_lm.fit(data, epochs=20, batch_size=1)
```
